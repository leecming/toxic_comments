{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "TARGET_COLS = ['toxic',\n",
    "               'severe_toxic',\n",
    "               'obscene',\n",
    "               'threat',\n",
    "               'insult',\n",
    "               'identity_hate']\n",
    "\n",
    "train_df = pd.read_csv('../data/train.csv').sort_values(by='id')\n",
    "bert_preds = pd.read_csv('../data/preds_bert.csv').sort_values(by='id')\n",
    "gpt_preds = pd.read_csv('../data/preds_gpt.csv').sort_values(by='id')\n",
    "bigru_fasttext_preds = pd.read_csv('../data/preds_bigru_fasttext_base.csv').sort_values(by='id')\n",
    "bigru_elmo_preds = pd.read_csv('../data/preds_bigru_elmo_base.csv').sort_values(by='id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# Confirm that the prediction and train set ids are one-to-one \n",
    "assert set(train_df.id) == set(bert_preds.id)\n",
    "assert set(train_df.id) == set(gpt_preds.id)\n",
    "assert set(train_df.id) == set(bigru_fasttext_preds.id)\n",
    "assert set(train_df.id) == set(bigru_elmo_preds.id)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "BERT ROC-AUC: 0.9894\n",
      "GPT ROC-AUC: 0.9880\n",
      "FT BiGRU ROC-AUC: 0.9858\n",
      "ELMo BiGRU ROC-AUC: 0.9867\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "print('BERT ROC-AUC: {0:.4f}'.format(roc_auc_score(train_df[TARGET_COLS].values, bert_preds[TARGET_COLS].values)))\n",
    "print('GPT ROC-AUC: {0:.4f}'.format(roc_auc_score(train_df[TARGET_COLS].values, gpt_preds[TARGET_COLS].values)))\n",
    "print('FT BiGRU ROC-AUC: {0:.4f}'.format(roc_auc_score(train_df[TARGET_COLS].values, bigru_fasttext_preds[TARGET_COLS].values)))\n",
    "print('ELMo BiGRU ROC-AUC: {0:.4f}'.format(roc_auc_score(train_df[TARGET_COLS].values, bigru_elmo_preds[TARGET_COLS].values)))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Average ensembler ROC-AUC: 0.9921\n",
      "Geo mean ensembler ROC-AUC: 0.9917\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats.mstats import gmean\n",
    "simple_avg = np.mean(np.stack([bert_preds[TARGET_COLS].values,\n",
    "                               gpt_preds[TARGET_COLS].values,\n",
    "                               bigru_fasttext_preds[TARGET_COLS].values,\n",
    "                               bigru_elmo_preds[TARGET_COLS].values]), \n",
    "                     axis=0)\n",
    "geo_avg = gmean(np.stack([bert_preds[TARGET_COLS].values,\n",
    "                               gpt_preds[TARGET_COLS].values,\n",
    "                               bigru_fasttext_preds[TARGET_COLS].values,\n",
    "                               bigru_elmo_preds[TARGET_COLS].values]),\n",
    "                axis=0)\n",
    "\n",
    "print('Average ensembler ROC-AUC: {0:.4f}'.format(roc_auc_score(train_df[TARGET_COLS].values,\n",
    "                                                                simple_avg)))\n",
    "print('Geo mean ensembler ROC-AUC: {0:.4f}'.format(roc_auc_score(train_df[TARGET_COLS].values,\n",
    "                                                                geo_avg)))\n",
    "\n",
    "soft_col_labels = list(map(lambda x: 'soft_'+x, TARGET_COLS))\n",
    "avg_df = pd.DataFrame(simple_avg, columns=soft_col_labels)\n",
    "# pd.concat([train_df.reset_index(drop=True), avg_df.reset_index(drop=True)], axis=1).to_csv('../data/train_with_soft_targets.csv',index=False)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "BERT hard + soft ROC-AUC: 0.9702\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "bert_hardsoft_df = pd.read_csv('../data/preds_bert_hardsoft_targets.csv').sort_values(by='id')\n",
    "print('BERT hard + soft ROC-AUC: {0:.4f}'.format(roc_auc_score(train_df[TARGET_COLS].values, bert_hardsoft_df[TARGET_COLS].values)))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}